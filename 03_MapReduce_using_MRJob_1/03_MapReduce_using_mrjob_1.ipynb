{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce Using `MRJob`: Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Posting Dataset\n",
    "\n",
    "The sample dataset we will mainly use (`data/job-data/job-data-2018-09-*.txt`) for this tutorial contains job postings from one of the US job search websites. The data is stored with each row as a JSON document representing a job posting record. \n",
    "\n",
    "The example below shows a sample job postings from the data file. The sample record has been formatted with 4 spaces indentation. In the real file, each record is stored as a JSON document in one row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example: JSON document of a job posting record*\n",
    "\n",
    "```\n",
    "{\n",
    "    \"industry\": \"Information Technology\", \n",
    "    \"datePosted\": \"2018-09-07\", \n",
    "    \"salaryCurrency\": \"USD\", \n",
    "    \"validThrough\": \"2018-10-07\", \n",
    "    \"empId\": 671932, \n",
    "    \"jobLocation\": {\n",
    "        \"geo\": {\n",
    "            \"latitude\": \"37.7623\", \n",
    "            \"@type\": \"GeoCoordinates\", \n",
    "            \"longitude\": \"-122.4145\"\n",
    "        }, \n",
    "        \"@type\": \"Place\", \n",
    "        \"address\": {\n",
    "            \"postalCode\": \"94110-2042\", \n",
    "            \"addressLocality\": \"San Francisco\", \n",
    "            \"@type\": \"PostalAddress\", \n",
    "            \"addressRegion\": \"CA\", \n",
    "            \"addressCountry\": {\n",
    "                \"@type\": \n",
    "                \"Country\", \n",
    "                \"name\": \"US\"\n",
    "            }\n",
    "        }\n",
    "    }, \n",
    "    \"estimatedSalary\": {\n",
    "        \"@type\": \"MonetaryAmount\", \n",
    "        \"currency\": \"USD\", \n",
    "        \"value\": {\n",
    "            \"maxValue\": \"202000\", \n",
    "            \"@type\": \"QuantitativeValue\", \n",
    "            \"unitText\": \"YEAR\", \n",
    "            \"minValue\": \"146000\"\n",
    "        }\n",
    "    }, \n",
    "    \"description\": \"<div><em>Generate insights and impact from data</em><em>.</em></div>\\n<br/>\\n<div>\\n<div>We're looking for data scientists to join the Analytics team who are excited about applying their analytical skills to understand our users and influence decision making. If you are naturally data curious, excited about deriving insights from data, and motivated by having impact on the business, we want to hear from you.</div><br/>\\n\\n<div><strong>You will:</strong></div><div>\\n\\n\\n<ul>\\n<li>Work closely with product and business teams to identify important questions and answer them with data.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Apply statistical and econometric models on large datasets to: i) measure results and outcomes, ii) identify causal impact and attribution, iii) predict future performance of users or products.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Design, analyze, and interpret the results of experiments.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Drive the collection of new data and the refinement of existing data sources.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Create analyses that tell a \\\"story\\\" focused on insights, not just data.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div><strong>We're looking for someone with:</strong></div><div>\\n\\n\\n<ul>\\n<li>3+ years experience working with and analyzing large data sets to solve problems.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>A PhD or MS in a quantitative field (e.g., Economics, Statistics, Eng, Natural Sciences, CS).</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Expert knowledge of a scientific computing language (such as R or Python) and SQL.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Strong knowledge of statistics and experimental design.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Ability to communicate results clearly and a focus on driving impact.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div><strong>Nice to haves:</strong></div><div>\\n\\n\\n<ul>\\n<li>Prior experience with data-distributed tools (Scalding, Hadoop, Pig, etc).</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div><strong>You should include these in your application:</strong></div><div>\\n\\n\\n<ul>\\n<li>Resume and LinkedIn profile.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Description of the most interesting data analysis you've done, key findings, and its impact.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Link to or attachment of code you've written related to data analysis.</li>\\n</ul>\\n\\n</div>\\n</div>\\n<br/>\", \n",
    "    \"hiringOrganization\": {\n",
    "        \"@type\": \"Organization\", \n",
    "        \"sameAs\": \"www.stripe.com\", \n",
    "        \"name\": \"Stripe\"\n",
    "    },\n",
    "    \"@type\": \"JobPosting\", \n",
    "    \"jobId\": 2280174543, \n",
    "    \"@context\": \"http://schema.org\", \n",
    "    \"employmentType\": \"FULL_TIME\", \n",
    "    \"occupationalCategory\": [\n",
    "        \"15-1111.00\", \n",
    "        \"Computer and Information Research Scientists\"\n",
    "    ], \n",
    "    \"title\": \"Data Scientist\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy input data to HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `job-data': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir job-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `job-data/job-data-2018-09-08.txt': File exists\r\n",
      "put: `job-data/job-data-2018-09-09.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put ../data/job-data/* job-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Protocols For Input & Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mrjob` assumes that all data is newline-delimited bytes. Each job has an *input protocol*, an *output protocol*, and an *internal protocol*. These protocols can be changed by overwritting the attributes: `INPUT_PROTOCOL`, `INTERNAL_PROTOCOL`, and `OUTPUT_PROTOCOL`, respectively.\n",
    "\n",
    "The default *input* protocol is `RawValueProtocol`, which just reads in a line as a `str`.\n",
    "The default *output* and *internal* protocols are both `JSONProtocol`, which reads and writes JSON strings separated by a tab character.\n",
    "\n",
    "`JSONValueProtocol` encodes value as a JSON and discard key (key is read in as None). To load the job posting dataset, we can set `INPUT_PROTOCOL = JSONValueProtocol` which automaticall loads input data as Python `dict` objects.\n",
    "\n",
    "For more information, see [Protocols](https://pythonhosted.org/mrjob/guides/writing-mrjobs.html#job-protocols)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**: Simple JSON Parser\n",
    "\n",
    "The script below reads the data into `MRTest.mapper` with each record loaded as a Python dict, and generates output of key-value pairs where keys are `jobId` and values are `jobLocation`, which will then be written into output files as JSON documents. Note that no `MRTest.reducer` is provided, this type of jobs are also called *map-only* jobs.\n",
    "\n",
    "- *Data flow*:\n",
    "\n",
    "  - Input:`record`\n",
    "  - $\\quad\\downarrow$\n",
    "  - mapper:`<_, record> -> <jobId, jobLocation>`\n",
    "  - $\\quad\\downarrow$\n",
    "  - Output:`jobId jobLocation`\n",
    "  \n",
    "- *Features and highlights*:\n",
    "  \n",
    "  `INPUT_PROTOCOL = JSONValueProtocol` allows MRJob to parse JSON documents as python dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mr-jobs/1_protocols.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/1_protocols.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "\n",
    "class MRTest(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "    def mapper(self, _, value):\n",
    "        yield value.get('jobId', None), value.get('jobLocation', None)\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRTest.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/1_protocols.hadoop.20180926.222546.611543\n",
      "job output is in mr-output\n",
      "Removing temp directory /tmp/1_protocols.hadoop.20180926.222546.611543...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/1_protocols.py ../data/job-data/* --output-dir mr-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/1_protocols.hadoop.20180926.222550.384342\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/1_protocols.hadoop.20180926.222550.384342/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar4154328930656747625/] [] /tmp/streamjob4721268306871442407.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537993323748_0013\n",
      "  Submitted application application_1537993323748_0013\n",
      "  The url to track the job: http://c8d937eb6693:8088/proxy/application_1537993323748_0013/\n",
      "  Running job: job_1537993323748_0013\n",
      "  Job job_1537993323748_0013 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1537993323748_0013 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=463148\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323482\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=463148\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=9454592\n",
      "\t\tTotal time spent by all map tasks (ms)=9233\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9233\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9233\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1260\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=115\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output records=1772\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=336502784\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=170393600\n",
      "\t\tVirtual memory (bytes) snapshot=3925311488\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/1_protocols.hadoop.20180926.222550.384342...\n",
      "Removing temp directory /tmp/1_protocols.hadoop.20180926.222550.384342...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/1_protocols.py -r hadoop \\\n",
    "hdfs:///user/hadoop/job-data/ \\\n",
    "--output-dir mr-output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**: Simple Event Counter\n",
    "\n",
    "With the help of MRJob, it's easy to write some simple mapreduce jobs. For example, to compute the number of jobs available in each region, the data flow becomes:\n",
    "\n",
    "- *Data flow*:\n",
    "\n",
    "  - Input:`record`\n",
    "  - $\\quad\\downarrow$\n",
    "  - mapper:`<_, record> -> <addressRegion, 1>`\n",
    "  - $\\quad\\downarrow$\n",
    "  - reducer:`<addressRegion, [1]> -> <addressRegion, count>`\n",
    "  - $\\quad\\downarrow$\n",
    "  - Output:`jobId jobLocation`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mr-jobs/1_region_counter.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/1_region_counter.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "\n",
    "class MRCounter(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "    def mapper(self, _, value):\n",
    "        try:\n",
    "            region = value['jobLocation']['address']['addressRegion']\n",
    "        except KeyError:\n",
    "            yield 'NA', 1\n",
    "        else:\n",
    "            yield region, 1\n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                       combiner=self.reducer,\n",
    "                       reducer=self.reducer)]\n",
    "    \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRCounter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/1_region_counter.hadoop.20180926.222629.230984\n",
      "job output is in mr-output\n",
      "Removing temp directory /tmp/1_region_counter.hadoop.20180926.222629.230984...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/1_region_counter.py ../data/job-data/* --output-dir mr-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/1_region_counter.hadoop.20180926.222632.974850\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/1_region_counter.hadoop.20180926.222632.974850/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar2848612009597891020/] [] /tmp/streamjob6857587264638180417.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537993323748_0014\n",
      "  Submitted application application_1537993323748_0014\n",
      "  The url to track the job: http://c8d937eb6693:8088/proxy/application_1537993323748_0014/\n",
      "  Running job: job_1537993323748_0014\n",
      "  Job job_1537993323748_0014 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1537993323748_0014 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=853\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1557\n",
      "\t\tFILE: Number of bytes written=491099\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=853\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8588288\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4739072\n",
      "\t\tTotal time spent by all map tasks (ms)=8387\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8387\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4628\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4628\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8387\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4628\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1810\n",
      "\t\tCombine input records=1772\n",
      "\t\tCombine output records=146\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=161\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output bytes=12702\n",
      "\t\tMap output materialized bytes=1563\n",
      "\t\tMap output records=1772\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=677888000\n",
      "\t\tReduce input groups=92\n",
      "\t\tReduce input records=146\n",
      "\t\tReduce output records=92\n",
      "\t\tReduce shuffle bytes=1563\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=292\n",
      "\t\tTotal committed heap usage (bytes)=471859200\n",
      "\t\tVirtual memory (bytes) snapshot=5887578112\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/1_region_counter.hadoop.20180926.222632.974850...\n",
      "Removing temp directory /tmp/1_region_counter.hadoop.20180926.222632.974850...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/1_region_counter.py -r hadoop \\\n",
    "hdfs:///user/hadoop/job-data/ \\\n",
    "--output-dir mr-output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "In this exercise we want to find the number of job postings in each industry.\n",
    "\n",
    "- Create a MapReduce script and define your MRJob class to count the number of job postings in each industry. \n",
    "- When you finish, first test it locally and check the output.\n",
    "- Run it on the hadoop cluster and check the HDFS directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filtering\n",
    "\n",
    "Keys:\n",
    "\n",
    "- Filtering pattern aims to find a subset of data but usually not change the actural records. \n",
    "  - We can set `OUTPUT_PROTOCOL = JSONValueProtocol` to ignore the key field for each record in the output.\n",
    "- Filtering patterns usually don't need a reducer if each record is filtered individually and the evaluation does not depend on other records.\n",
    "- Filtering usually serves as an abstract pattern for some other patterns.\n",
    "\n",
    "Applications:\n",
    "\n",
    "- Data cleaning\n",
    "- Events tracking\n",
    "- Records matching\n",
    "- Random sampling\n",
    "- Dataset splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Simple Filtering\n",
    "\n",
    "Simple filtering is often used when data cleaning, events tracking, outliers removing, etc., are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**: Find all jobs with titles relavant to *Data Scientist*.\n",
    "\n",
    "\n",
    "- *Data flow*:\n",
    "\n",
    "  - Input:`record`\n",
    "  - $\\quad\\downarrow$\n",
    "  - mapper:`<_, record> [if keyword in title -> <None, record>]`\n",
    "  - $\\quad\\downarrow$\n",
    "  - Output:`record`\n",
    "  \n",
    "- *Features and highlights*:\n",
    "  \n",
    "  `OUTPUT_PROTOCOL = JSONValueProtocol` ignores the key field for each record in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mr-jobs/2.1_simple_filtering.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/2.1_simple_filtering.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "\n",
    "class MRSimpleFiltering(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    \n",
    "    def mapper(self, _, value):\n",
    "        title = value.get('title', '').lower()\n",
    "        if title.find('data scientist') > -1:\n",
    "            yield _, value\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRSimpleFiltering.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/2.hadoop.20180926.222712.099042\n",
      "job output is in mr-output\n",
      "Removing temp directory /tmp/2.hadoop.20180926.222712.099042...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.1_simple_filtering.py ../data/job-data/* --output-dir mr-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/2.hadoop.20180926.222715.775770\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180926.222715.775770/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar2506588348652920310/] [] /tmp/streamjob2871258137806635665.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537993323748_0015\n",
      "  Submitted application application_1537993323748_0015\n",
      "  The url to track the job: http://c8d937eb6693:8088/proxy/application_1537993323748_0015/\n",
      "  Running job: job_1537993323748_0015\n",
      "  Job job_1537993323748_0015 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1537993323748_0015 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2928694\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323470\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=2928694\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=7553024\n",
      "\t\tTotal time spent by all map tasks (ms)=7376\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7376\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7376\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1050\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=90\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output records=657\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=338317312\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=169869312\n",
      "\t\tVirtual memory (bytes) snapshot=3931668480\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180926.222715.775770...\n",
      "Removing temp directory /tmp/2.hadoop.20180926.222715.775770...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.1_simple_filtering.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "--output-dir mr-output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Sampling\n",
    "\n",
    "Random sampling pattern allows us to create a subset (usually much smaller) of our larger dataset for quick exploration. Thus each record should have an equal probability of being selected. \n",
    "\n",
    "If reproducible is not required, then we can use a random function, e.g.: `random.uniform(a, b)` in python, to do the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**: Create a random subset with 10% of the full dataset.\n",
    "\n",
    "We want to pass an argument `fraction` to our `MRJob` script. We can do this by using `MRJob.configure_args()` and `MRJob.add_passthru_arg()` together.\n",
    "\n",
    "- *Data flow*:\n",
    "\n",
    "  - Input:`record`\n",
    "  - $\\quad\\downarrow$\n",
    "  - mapper:`<_, record> [Prob=0.1 -> <None, record>]`\n",
    "  - $\\quad\\downarrow$\n",
    "  - Output:`record`\n",
    "  \n",
    "- *Features and highlights*:\n",
    "  \n",
    "  - `MRJob.configure_args()` allows user to define arguments for this script \n",
    "  - `MRJob.add_passthru_arg('--fraction', **kwargs)` defines a command-line argument named `fraction`\n",
    "  - To pass a value to `fraction` via command-line arguemnt: `--fraction <value>`\n",
    "  - To use `fraction` in script: `MRJob.options.fraction`\n",
    "  - `MRJob.mapper_init()` validates the value of `fraction` before the `mapper` processes any input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mr-jobs/2.2_random_sampling.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/2.2_random_sampling.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "class MRRandomSampling(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    \n",
    "    def configure_args(self):\n",
    "        super().configure_args()\n",
    "        self.add_passthru_arg('--fraction', type=float)\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        if self.options.fraction > 1 or self.options.fraction < 0:\n",
    "            raise ValueError('Invalid fraction value')\n",
    "        \n",
    "    def mapper(self, _, value):\n",
    "        if random.uniform(0, 1) < self.options.fraction:\n",
    "            yield _, value\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRRandomSampling.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/2.hadoop.20180926.222749.971923\n",
      "job output is in mr-output/\n",
      "Removing temp directory /tmp/2.hadoop.20180926.222749.971923...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.2_random_sampling.py ../data/job-data/* --output-dir mr-output/ --fraction .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/2.hadoop.20180926.222753.492256\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180926.222753.492256/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar7073738273186572016/] [] /tmp/streamjob773106399113576008.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537993323748_0016\n",
      "  Submitted application application_1537993323748_0016\n",
      "  The url to track the job: http://c8d937eb6693:8088/proxy/application_1537993323748_0016/\n",
      "  Running job: job_1537993323748_0016\n",
      "  Job job_1537993323748_0016 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1537993323748_0016 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=865768\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323484\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=865768\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=7572480\n",
      "\t\tTotal time spent by all map tasks (ms)=7395\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7395\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7395\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=940\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=105\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output records=184\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=336912384\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=175636480\n",
      "\t\tVirtual memory (bytes) snapshot=3929145344\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180926.222753.492256...\n",
      "Removing temp directory /tmp/2.hadoop.20180926.222753.492256...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.2_random_sampling.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "--output-dir mr-output/ --fraction .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data Splitting\n",
    "\n",
    "For machine learning modeling, we usually divide the data set into two non-overlapping subsets:\n",
    "- training set — a subset to train a model.\n",
    "- test set — a subset to test the trained model.\n",
    "\n",
    "If the goal is to split the dataset into such two subsets, then we need to make sure:\n",
    "- each record can only be selected into one of the two datasets\n",
    "- sampling is reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sample` function below returns either `True` or `False` based on the hashed value of key and fraction:\n",
    "1. split fraction into *numerator* and *denominator*, e.g.: 0.125 $\\rightarrow$ 125/1000\n",
    "2. calculate the hash value of the key. Here we will use MD5, which is a widely used hash function producing a 128-bit hash value.\n",
    "3. calculate hash value modulo *denominator*, if it's less than *numerator*, return `True`, otherwise return `False`.\n",
    "\n",
    "Note: if you just want to randomly sample the dataset, then a simple random number generator will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "import hashlib\n",
    "\n",
    "def sample(key, fraction):\n",
    "    if fraction > 1 or fraction < 0:\n",
    "        raise ValueError('Invalid fraction value')\n",
    "    # calculate numerator and denominator\n",
    "    frac = decimal.Decimal(str(fraction)).as_tuple()\n",
    "    numer = sum([v*10**i for i, v in enumerate(frac.digits[::-1])])\n",
    "    denom = 10**(-frac.exponent)\n",
    "    # calculate hash value using md5\n",
    "    hash_val = hashlib.md5(str(key).encode()).hexdigest()\n",
    "    return (int(hash_val, 16) % denom) < numer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n"
     ]
    }
   ],
   "source": [
    "# test the function with the code below\n",
    "N = 1000\n",
    "print(sum([sample(i, fraction=0.25) for i in range(N)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**: Creating a reproducible train/test split.\n",
    " \n",
    "- *Features and highlights*:\n",
    "    \n",
    "  - `MRJob.add_passthru_arg('--split')` defines a command-line argument named `split`, which takes value \"train\" or \"test\". \n",
    "    - If `split=train`, it outputs train subset, otherwise it outputs test subset.\n",
    "  - `MRJob.add_passthru_arg('--test_size')` defines a command-line argument named `test_size`. \n",
    "    - The value should be between 0.0 and 1.0, which represent the proportion of the dataset to include in the test split.\n",
    "  - To create a train/test split, we run the script twice, one with `split=train` and one with `split=test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mr-jobs/2.3_train_test_splitting.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/2.3_train_test_splitting.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "import decimal\n",
    "import hashlib\n",
    "\n",
    "\n",
    "class MRTrainTestSplit(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    \n",
    "    def configure_args(self):\n",
    "        super().configure_args()\n",
    "        self.add_passthru_arg('--split')\n",
    "        self.add_passthru_arg('--test_size', type=float, default=0.3)\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        if self.options.split not in ('train', 'test'):\n",
    "            raise ValueError('Invalid split value')\n",
    "        if self.options.test_size > 1 or self.options.test_size < 0:\n",
    "            raise ValueError('Invalid test size')\n",
    "        \n",
    "    def mapper(self, _, value):\n",
    "        key = value.get('jobId', 0)\n",
    "        include = self._sample(key=key, fraction=self.options.test_size)\n",
    "        if include ^ (self.options.split=='train'):\n",
    "            yield _, value\n",
    "    \n",
    "    def _sample(self, key, fraction=1):\n",
    "        frac = decimal.Decimal(str(fraction)).as_tuple()\n",
    "        numer = sum([v*10**i for i, v in enumerate(frac.digits[::-1])])\n",
    "        denom = 10**(-frac.exponent)\n",
    "        hash_val = hashlib.md5(str(key).encode()).hexdigest()\n",
    "        return (int(hash_val, 16) % denom) < numer\n",
    "    \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRTrainTestSplit.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/2.hadoop.20180926.222826.701787\n",
      "job output is in mr-output/train\n",
      "Removing temp directory /tmp/2.hadoop.20180926.222826.701787...\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/2.hadoop.20180926.222827.538116\n",
      "job output is in mr-output/test\n",
      "Removing temp directory /tmp/2.hadoop.20180926.222827.538116...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.3_train_test_splitting.py ../data/job-data/* \\\n",
    "--output-dir mr-output/train \\\n",
    "--test_size 0.3 \\\n",
    "--split train \\\n",
    "&& python3 mr-jobs/2.3_train_test_splitting.py ../data/job-data/* \\\n",
    "--output-dir mr-output/test \\\n",
    "--test_size 0.3 \\\n",
    "--split test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/2.hadoop.20180926.222831.229682\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180926.222831.229682/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6197470053724491520/] [] /tmp/streamjob7920787548911979230.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537993323748_0017\n",
      "  Submitted application application_1537993323748_0017\n",
      "  The url to track the job: http://c8d937eb6693:8088/proxy/application_1537993323748_0017/\n",
      "  Running job: job_1537993323748_0017\n",
      "  Job job_1537993323748_0017 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1537993323748_0017 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/train\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6282102\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323592\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=6282102\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8750080\n",
      "\t\tTotal time spent by all map tasks (ms)=8545\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8545\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8545\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1180\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=114\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output records=1270\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=335917056\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=181403648\n",
      "\t\tVirtual memory (bytes) snapshot=3929567232\n",
      "job output is in hdfs:///user/hadoop/mr-output/train\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180926.222831.229682...\n",
      "Removing temp directory /tmp/2.hadoop.20180926.222831.229682...\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/2.hadoop.20180926.222904.818624\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180926.222904.818624/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar3887483473099055096/] [] /tmp/streamjob179215724187321896.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537993323748_0018\n",
      "  Submitted application application_1537993323748_0018\n",
      "  The url to track the job: http://c8d937eb6693:8088/proxy/application_1537993323748_0018/\n",
      "  Running job: job_1537993323748_0018\n",
      "  Job job_1537993323748_0018 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1537993323748_0018 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/test\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2569476\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323586\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=2569476\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=9412608\n",
      "\t\tTotal time spent by all map tasks (ms)=9192\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9192\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9192\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1100\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=110\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output records=502\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=344653824\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=171442176\n",
      "\t\tVirtual memory (bytes) snapshot=3934253056\n",
      "job output is in hdfs:///user/hadoop/mr-output/test\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180926.222904.818624...\n",
      "Removing temp directory /tmp/2.hadoop.20180926.222904.818624...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.3_train_test_splitting.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "    --output-dir mr-output/train \\\n",
    "    --test_size 0.3 \\\n",
    "    --split train \\\n",
    "&& python3 mr-jobs/2.3_train_test_splitting.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "    --output-dir mr-output/test \\\n",
    "    --test_size 0.3 \\\n",
    "    --split test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2**\n",
    "\n",
    "You may have already noticed from the output of exercise 1 that the majority jobs belong to *Information Technology* industry. \n",
    "\n",
    "- Now create another script to produce a 30% sample from all jobs whose industry is `\"Information Technology\"`.\n",
    "- When you finish, test it locally and on the hadoop cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
